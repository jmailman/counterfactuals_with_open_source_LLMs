# counterfactuals_with_open_source_LLMs
This is a basic exploration of how some open source LLMs fare when counterfactual questions are posed to them. Rather than being exhaustive and systematic, this exploration is more anecdotal. 

### Process and tooling
Ollama
Docker

### Table of Contents

### Additional

#### How the project came about

#### Motivation
Running models locally (as opposed to API calls to proprietary models)

#### Limitations

#### Challenges
Ollama runs as a service. Locally it can run in a Docker container. But on Databricks things are more complicated.

#### The problem this addresses

#### Intended use

#### Credits
